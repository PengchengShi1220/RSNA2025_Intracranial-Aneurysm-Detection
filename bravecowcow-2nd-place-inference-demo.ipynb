{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b7928f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T05:29:23.949917Z",
     "iopub.status.busy": "2025-10-22T05:29:23.949181Z",
     "iopub.status.idle": "2025-10-22T05:29:33.832820Z",
     "shell.execute_reply": "2025-10-22T05:29:33.831951Z"
    },
    "papermill": {
     "duration": 9.889418,
     "end_time": "2025-10-22T05:29:33.834317",
     "exception": false,
     "start_time": "2025-10-22T05:29:23.944899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/wheels-20251001/wheels_20251001/connected_components_3d-3.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/wheels-20251001/wheels_20251001/monai-1.5.0-py3-none-any.whl\r\n",
      "Installing collected packages: monai, connected-components-3d\r\n",
      "Successfully installed connected-components-3d-3.24.0 monai-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/wheels-20251001/wheels_20251001/*.whl --no-deps\n",
    "!cp -r /kaggle/input/nnxnet-050/nnXNet_050 /kaggle/nnxnet\n",
    "!cp -r /kaggle/input/wheels-20251001/wheels_20251001/dicom2nifti_20250917 /kaggle/dicom2nifti\n",
    "!cp -r /kaggle/input/wheels-20251001/wheels_20251001/acvl_utils-0.2.5 /kaggle/acvl_utils\n",
    "!cp -r /kaggle/input/wheels-20251001/wheels_20251001/batchgenerators-0.25.1 /kaggle/batchgenerators\n",
    "!cp -r /kaggle/input/wheels-20251001/wheels_20251001/dynamic_network_architectures-0.3.1 /kaggle/dynamic_network_architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d052e9f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T05:29:33.841027Z",
     "iopub.status.busy": "2025-10-22T05:29:33.840779Z",
     "iopub.status.idle": "2025-10-22T05:29:33.844956Z",
     "shell.execute_reply": "2025-10-22T05:29:33.844412Z"
    },
    "papermill": {
     "duration": 0.008581,
     "end_time": "2025-10-22T05:29:33.845990",
     "exception": false,
     "start_time": "2025-10-22T05:29:33.837409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/nnxnet')\n",
    "sys.path.append('/kaggle/dicom2nifti')\n",
    "sys.path.append('/kaggle/acvl_utils')\n",
    "sys.path.append('/kaggle/batchgenerators')\n",
    "sys.path.append('/kaggle/dynamic_network_architectures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0abb86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T05:29:33.852748Z",
     "iopub.status.busy": "2025-10-22T05:29:33.852557Z",
     "iopub.status.idle": "2025-10-22T05:30:35.218580Z",
     "shell.execute_reply": "2025-10-22T05:30:35.217933Z"
    },
    "papermill": {
     "duration": 61.371549,
     "end_time": "2025-10-22T05:30:35.219994",
     "exception": false,
     "start_time": "2025-10-22T05:29:33.848445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnXNet_raw is not defined and nnX-Net can only be used on data for which preprocessed files are already present on your system. nnX-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
      "nnXNet_preprocessed is not defined and nnX-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "nnXNet_results is not defined and nnX-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# RSNA Intracranial Aneurysm Challenge - Inference\n",
    "# Two-stage pipeline: DICOM → NIfTI → Brain Segmentation → Aneurysm Classification\n",
    "# ==================================================\n",
    "\"\"\"HOUJING:\n",
    "ONLY WORK FOR 2 GPUs.\n",
    "How inference tasks are divided between workers are defined by how you call `executor.submit`.\n",
    "Only create the thread pool once and use it for all case prediction, because creating the pool for each case is very time consuming.\n",
    "Extension:\n",
    "If you want to use 4 folds with 2 GPUs:\n",
    "- Submit fold_0 and fold_1 prediction in parallel\n",
    "- Wait till finish\n",
    "- Submit fold_2 and fold_3 prediction in parallel\n",
    "- Wait till finish\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pydicom\n",
    "import shutil\n",
    "import gc\n",
    "import nibabel as nib\n",
    "import dicom2nifti\n",
    "import kaggle_evaluation.rsna_inference_server\n",
    "import torch.nn.functional as F\n",
    "from skimage.transform import resize\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union, List, Tuple, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from nnxnet.inference.predict_from_raw_data_2D_orthogonal_planes_fast import nnXNetPredictor\n",
    "from nnxnet.inference.predict_from_raw_data_two_seg_with_cls_no_seg_return_no_filter import nnXNetPredictor as nnXNetPredictorWithCls\n",
    "from nnxnet.utilities.helpers import empty_cache, dummy_context\n",
    "\n",
    "# Constants\n",
    "MODEL_PATHS = {\n",
    "    'vessel_ROI_seg': \"/kaggle/input/dataset180_2d_vessel_box_seg_stable/pytorch/default/1/Dataset180_2D_vessel_box_seg_stable/nnUNetTrainer__nnUNetPlans__2d\",\n",
    "    'aneurysm_cls_1': \"/kaggle/input/rsna2025-stage2-models/pytorch/default/4/RSNA2025_stage2_models/onlyMirror01_lr4e3_100epochs\",\n",
    "    'aneurysm_cls_2': \"/kaggle/input/rsna2025-stage2-models/pytorch/default/4/RSNA2025_stage2_models/onlyMirror01_250epochs\",\n",
    "    'plane_2d_cls': \"/kaggle/input/resnet34_plane_2d_cls/pytorch/default/1/ResNet34_Plane_2D_cls/checkpoint_best_loss.pth\"\n",
    "}\n",
    "SHARED_DIR = Path('/kaggle/shared')\n",
    "TEMP_DIR = Path('/kaggle/working')\n",
    "ID_COL = 'SeriesInstanceUID'\n",
    "LABEL_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery',\n",
    "    'Right Infraclinoid Internal Carotid Artery',\n",
    "    'Left Supraclinoid Internal Carotid Artery',\n",
    "    'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery',\n",
    "    'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery',\n",
    "    'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery',\n",
    "    'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery',\n",
    "    'Basilar Tip',\n",
    "    'Other Posterior Circulation',\n",
    "    'Aneurysm Present',\n",
    "]\n",
    "\n",
    "# Define global variables at the module level\n",
    "GLOBAL_VESSEL_ROI_PREDICTOR = None\n",
    "GLOBAL_ANEURYSM_PREDICTOR_ALL_FOLDS = None\n",
    "CLS_2D_PREDICTOR = None\n",
    "\n",
    "USE_NUM_GPUS = 2\n",
    "NUM_INFER_WORKERS = 2\n",
    "COMPILE_NETWORK = False\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=NUM_INFER_WORKERS)\n",
    "\n",
    "def get_device(gpu_id: int = 0) -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the computation device, with validation for GPU availability.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():\n",
    "        return torch.device(f\"cuda:{gpu_id}\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# Get the device (this part of the original code is fine)\n",
    "DEVICE = get_device(gpu_id=0)\n",
    "\n",
    "# ==================================================\n",
    "# Plane Classification Model Definition\n",
    "# ==================================================\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, block, num_blocks, in_channels=1):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.block = block\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) \n",
    "        self.embed_dim = 512 * block.expansion\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionPooling(nn.Module):\n",
    "    def __init__(self, embed_dim, query_num, num_classes, num_heads=4, dropout=0.0):\n",
    "        super(CrossAttentionPooling, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.query_num = query_num\n",
    "        self.class_query = nn.Parameter(torch.randn(query_num, embed_dim))\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=False\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(query_num * embed_dim, num_classes) \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.class_query)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        \n",
    "        for name, param in self.cross_attention.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.flatten(2)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        query = self.class_query.unsqueeze(1).repeat(1, batch_size, 1)\n",
    "        \n",
    "        attended, _ = self.cross_attention(query=query, key=x, value=x)\n",
    "        \n",
    "        attended = self.norm(attended)\n",
    "        attended = self.dropout(attended)\n",
    "        attended_permuted = attended.permute(1, 0, 2)\n",
    "        attended_flatten = attended_permuted.flatten(1)\n",
    "        logits = self.classifier(attended_flatten) \n",
    "        return logits\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embed_dim, query_num, num_classes, dropout=0.0, use_cross_attention=True, num_heads=4):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        if use_cross_attention:\n",
    "            self.pooling = CrossAttentionPooling(\n",
    "                embed_dim=embed_dim, \n",
    "                query_num=query_num, \n",
    "                num_classes=num_classes, \n",
    "                num_heads=num_heads, \n",
    "                dropout=dropout\n",
    "            )\n",
    "        else:\n",
    "            self.pooling = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1), \n",
    "                nn.Flatten(1), \n",
    "                nn.Dropout(dropout), \n",
    "                nn.Linear(embed_dim, num_classes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pooling(x)\n",
    "\n",
    "\n",
    "class PlaneResNet34(nn.Module):\n",
    "    \"\"\"Single-task model: Plane classification only (3 classes: AX/SAG/COR)\"\"\"\n",
    "    \n",
    "    def __init__(self, dropout: float = 0.1):\n",
    "        super(PlaneResNet34, self).__init__()\n",
    "        \n",
    "        self.encoder = ResNetEncoder(BasicBlock, [3, 4, 6, 3], in_channels=1)\n",
    "        self.embed_dim = self.encoder.embed_dim\n",
    "        \n",
    "        self.head_plane = ClassificationHead(\n",
    "            embed_dim=self.embed_dim, \n",
    "            query_num=3, \n",
    "            num_classes=3, \n",
    "            dropout=dropout, \n",
    "            use_cross_attention=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        logits = self.head_plane(features)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ==================================================\n",
    "# Plane Classification Predictor\n",
    "# ==================================================\n",
    "\n",
    "class PlaneClassifier:\n",
    "    \"\"\"Axial Slice Plane Prediction\"\"\"\n",
    "    \n",
    "    # Plane Category Mapping\n",
    "    PLANE_MAP = {0: 'AX', 1: 'SAG', 2: 'COR'}\n",
    "    \n",
    "    def __init__(self, checkpoint_path: str, device: str = 'cuda:0', target_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Initialize the inferencer.\n",
    "        \n",
    "        Args:\n",
    "            checkpoint_path: Path to the model weights.\n",
    "            device: Inference device (e.g., 'cuda', 'cpu').\n",
    "            target_size: Target image dimensions.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.target_size = target_size\n",
    "        self.model = self._load_model(checkpoint_path)\n",
    "    \n",
    "    def _load_model(self, checkpoint_path: str) -> PlaneResNet34:\n",
    "        \"\"\"Load model - Modified: Using PlaneResNet34\"\"\"\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Model file does not exist: {checkpoint_path}\")\n",
    "        \n",
    "        model = PlaneResNet34(dropout=0.0)\n",
    "        \n",
    "        # Load model weights\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=True)\n",
    "        \n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def preprocess_slice(self, slice_2d: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Preprocesses a 2D slice.\"\"\"\n",
    "        # 1. Type conversion and clipping\n",
    "        slice_data = slice_2d\n",
    "        \n",
    "        # 2. Z-score normalization\n",
    "        mean = slice_data.mean()\n",
    "        std = slice_data.std()\n",
    "        std = np.clip(std, 1e-8, None)\n",
    "        slice_data = (slice_data - mean) / std\n",
    "        \n",
    "        # 3. Resize\n",
    "        resized_slice = resize(slice_data, self.target_size, anti_aliasing=True).astype(np.float32)\n",
    "        \n",
    "        # 4. Convert to Tensor\n",
    "        tensor = torch.from_numpy(resized_slice).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def predict(self, tensor: torch.Tensor) -> Tuple[str, int, float, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs Plane prediction - Updated: adapted for single-task model output format\n",
    "        \n",
    "        Args:\n",
    "            tensor: Input tensor of shape [1, 1, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            plane_pred_label: Predicted class name ('AX', 'SAG', 'COR')\n",
    "            plane_pred: Predicted class index (0, 1, 2)\n",
    "            plane_prob: Maximum probability value\n",
    "            plane_prob_list: Probability distribution over all classes [1, 3]\n",
    "        \"\"\"\n",
    "        tensor = tensor.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(tensor)\n",
    "        \n",
    "        # Plane prediction\n",
    "        plane_pred = logits.argmax(dim=1).item()\n",
    "        plane_prob_list = F.softmax(logits, dim=1)\n",
    "        plane_prob = plane_prob_list.max().item()\n",
    "        plane_pred_label = self.PLANE_MAP[plane_pred]\n",
    "        \n",
    "        return plane_pred_label, plane_pred, plane_prob, plane_prob_list\n",
    "    \n",
    "    def inference_from_slice(\n",
    "        self, \n",
    "        slice_2d: np.ndarray,\n",
    "    ) -> Tuple[str, int, float, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs inference from a 2D slice.\n",
    "        \n",
    "        Returns:\n",
    "            plane_pred_label: Predicted class name\n",
    "            plane_pred: Predicted class index\n",
    "            plane_prob: Maximum probability\n",
    "            plane_prob_list: Probability distribution\n",
    "        \"\"\"\n",
    "        # 1. Preprocessing\n",
    "        tensor = self.preprocess_slice(slice_2d)\n",
    "        \n",
    "        # 2. Inference\n",
    "        plane_pred_label, plane_pred, plane_prob, plane_prob_list = self.predict(tensor)\n",
    "        \n",
    "        # 3. Print results\n",
    "        self._print_result(plane_pred_label, plane_pred, plane_prob, plane_prob_list)\n",
    "        \n",
    "        return plane_pred_label, plane_pred, plane_prob, plane_prob_list\n",
    "    \n",
    "    def _print_result(self, plane_pred_label: str, plane_pred: int, plane_prob: float, plane_prob_list: torch.Tensor):\n",
    "        \"\"\"Prints the prediction results.\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Plane Prediction Results:\")\n",
    "        print(f\"  Predicted Class: {plane_pred_label}\")\n",
    "        print(f\"  Class Index: {plane_pred}\")\n",
    "        print(f\"  Confidence: {plane_prob:.4f} ({plane_prob*100:.2f}%)\")\n",
    "        print(f\"  Probability Distribution: {plane_prob_list}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "def correct_orientation(pixel_array, spacing, plane_id):\n",
    "    \"\"\"\n",
    "    Corrects image orientation to standard axial view.\n",
    "    \"\"\"\n",
    "    if plane_id == 1:\n",
    "        # Sagittal → Axial\n",
    "        fixed_array = np.transpose(pixel_array, (1, 2, 0))\n",
    "        fixed_array = fixed_array[::-1, :, :]\n",
    "        fixed_spacing = [spacing[1], spacing[2], spacing[0]]\n",
    "        print(f\"  Corrected: {pixel_array.shape} → {fixed_array.shape}\")\n",
    "        print(f\"  Corrected: {spacing} → {fixed_spacing}\")\n",
    "        return fixed_array, fixed_spacing\n",
    "\n",
    "    elif plane_id == 2:\n",
    "        # Coronal → Axial\n",
    "        fixed_array = np.transpose(pixel_array, (1, 0, 2))\n",
    "        fixed_array = fixed_array[::-1, :, :]\n",
    "        fixed_spacing = [spacing[1], spacing[0], spacing[2]]\n",
    "        print(f\"  Corrected: {pixel_array.shape} → {fixed_array.shape}\")\n",
    "        print(f\"  Corrected: {spacing} → {fixed_spacing}\")\n",
    "        return fixed_array, fixed_spacing\n",
    "    else:\n",
    "        # Already axial or no correction needed\n",
    "        return pixel_array, spacing\n",
    "#=====================================================\n",
    "\n",
    "def reorient_nii(orig_nii, targ_aff=\"LPS\"):\n",
    "    \"\"\"\n",
    "    Reorient to the standard LPS+ DICOM coord.\n",
    "    \"\"\"\n",
    "    if \"\".join(nib.aff2axcodes(orig_nii.affine)) == targ_aff:\n",
    "        return orig_nii\n",
    "    orig_ornt = nib.io_orientation(orig_nii.affine)\n",
    "    targ_ornt = nib.orientations.axcodes2ornt(targ_aff)\n",
    "    transform = nib.orientations.ornt_transform(orig_ornt, targ_ornt)\n",
    "    img_orient = orig_nii.as_reoriented(transform)\n",
    "    return img_orient\n",
    "\n",
    "# ==================================================\n",
    "# 1. Initialize Model (Global One-time Initialization)\n",
    "# ==================================================\n",
    "def may_compile_network(network):\n",
    "    if COMPILE_NETWORK:\n",
    "        return torch.compile(network)\n",
    "    return network\n",
    "\n",
    "def init_predictors(device):\n",
    "    global GLOBAL_VESSEL_ROI_PREDICTOR, GLOBAL_ANEURYSM_PREDICTOR_ALL_FOLDS, CLS_2D_PREDICTOR\n",
    "    \n",
    "    # If already initialized, return the global instance directly\n",
    "    if GLOBAL_VESSEL_ROI_PREDICTOR is not None and GLOBAL_ANEURYSM_PREDICTOR_ALL_FOLDS is not None and CLS_2D_PREDICTOR is not None:\n",
    "        return GLOBAL_VESSEL_ROI_PREDICTOR, GLOBAL_ANEURYSM_PREDICTOR_ALL_FOLDS, CLS_2D_PREDICTOR\n",
    "    \n",
    "    # Stage 1: Vessel ROI Segmentation (Using the new multiplanar predictor)\n",
    "    GLOBAL_VESSEL_ROI_PREDICTOR = nnXNetPredictor(\n",
    "        tile_step_size=0.5,\n",
    "        use_mirroring=False,\n",
    "        use_gaussian=True,\n",
    "        perform_everything_on_device=True,\n",
    "        device=device,\n",
    "        allow_tqdm=False\n",
    "    )\n",
    "    GLOBAL_VESSEL_ROI_PREDICTOR.initialize_from_trained_model_folder(\n",
    "        model_training_output_dir=MODEL_PATHS['vessel_ROI_seg'],\n",
    "        use_folds=(0,),\n",
    "        checkpoint_name='checkpoint_final.pth',\n",
    "    )\n",
    "    GLOBAL_VESSEL_ROI_PREDICTOR.initialize_network_and_gaussian()\n",
    "    GLOBAL_VESSEL_ROI_PREDICTOR.network = may_compile_network(GLOBAL_VESSEL_ROI_PREDICTOR.network)\n",
    "\n",
    "    # Stage2: Aneurysm classification\n",
    "    aneurysm_predictor_f0 = nnXNetPredictorWithCls(\n",
    "        tile_step_size=0.5,\n",
    "        use_gaussian=True,\n",
    "        use_mirroring=False,\n",
    "        perform_everything_on_device=True,\n",
    "        device=get_device(gpu_id=0),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    aneurysm_predictor_f1 = nnXNetPredictorWithCls(\n",
    "        tile_step_size=0.5,\n",
    "        use_gaussian=True,\n",
    "        use_mirroring=False,\n",
    "        perform_everything_on_device=True,\n",
    "        device=get_device(gpu_id=1 if USE_NUM_GPUS == 2 else 0),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    aneurysm_predictor_f0.initialize_from_trained_model_folder(\n",
    "        model_training_output_dir=MODEL_PATHS['aneurysm_cls_1'],\n",
    "        use_folds=(0, ),\n",
    "        checkpoint_name=\"checkpoint_final.pth\",\n",
    "    )\n",
    "    aneurysm_predictor_f0.initialize_network_and_gaussian()\n",
    "\n",
    "    aneurysm_predictor_f1.initialize_from_trained_model_folder(\n",
    "        model_training_output_dir=MODEL_PATHS['aneurysm_cls_2'],\n",
    "        use_folds=(1, ),\n",
    "        checkpoint_name=\"checkpoint_final.pth\",\n",
    "    )\n",
    "    aneurysm_predictor_f1.initialize_network_and_gaussian()\n",
    "\n",
    "    aneurysm_predictor_f0.network = may_compile_network(aneurysm_predictor_f0.network)\n",
    "    aneurysm_predictor_f1.network = may_compile_network(aneurysm_predictor_f1.network)\n",
    "\n",
    "    # ========== Initialize 2D Orientation Classifier Inferencer ==========\n",
    "    CLS_2D_PREDICTOR = PlaneClassifier(\n",
    "        checkpoint_path=MODEL_PATHS['plane_2d_cls'],\n",
    "        device=device,\n",
    "        target_size=(256, 256)\n",
    "    )\n",
    "    CLS_2D_PREDICTOR.model = may_compile_network(CLS_2D_PREDICTOR.model)\n",
    "\n",
    "    GLOBAL_ANEURYSM_PREDICTOR_ALL_FOLDS = [aneurysm_predictor_f0, aneurysm_predictor_f1]\n",
    "    return GLOBAL_VESSEL_ROI_PREDICTOR, GLOBAL_ANEURYSM_PREDICTOR_ALL_FOLDS, CLS_2D_PREDICTOR\n",
    "\n",
    "    \n",
    "GLOBAL_VESSEL_ROI_PREDICTOR, GLOBAL_ANEURYSM_PREDICTOR, CLS_2D_PREDICTOR = init_predictors(DEVICE)\n",
    "\n",
    "def group_dicom_files(study_folder_path: str) -> Dict[Tuple, List[str]]:\n",
    "    \"\"\"\n",
    "    Groups DICOM files into pseudo-series based on StudyInstanceUID, \n",
    "    FrameOfReferenceUID, Modality, and ImageOrientationPatient.\n",
    "    \"\"\"\n",
    "    dicom_groups = {}\n",
    "    dicom_files = []\n",
    "\n",
    "    for root, _, files in os.walk(study_folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.dcm', '.DCM')) or ('.' not in file and len(file) > 1):\n",
    "                dicom_files.append(os.path.join(root, file))\n",
    "\n",
    "    if not dicom_files:\n",
    "        print(f\"No DICOM files found in path: {study_folder_path}\")\n",
    "        return dicom_groups\n",
    "    \n",
    "    print(f\"Found {len(dicom_files)} files in total, starting grouping...\")\n",
    "\n",
    "    for file_path in dicom_files:\n",
    "        try:\n",
    "            ds = pydicom.dcmread(file_path, stop_before_pixels=True)\n",
    "            \n",
    "            study_uid = getattr(ds, 'StudyInstanceUID', 'NO_STUDY_UID')\n",
    "            frame_uid = getattr(ds, 'FrameOfReferenceUID', 'NO_FRAME_UID')\n",
    "            modality = getattr(ds, 'Modality', 'UNKNOWN')\n",
    "            \n",
    "            orientation = getattr(ds, 'ImageOrientationPatient', [0, 0, 0, 0, 0, 0])\n",
    "            orientation_key = tuple(np.round(orientation, 4)) \n",
    "\n",
    "            group_key = (study_uid, frame_uid, modality, orientation_key)\n",
    "\n",
    "            if group_key not in dicom_groups:\n",
    "                dicom_groups[group_key] = []\n",
    "            dicom_groups[group_key].append(file_path)\n",
    "\n",
    "        except (pydicom.errors.InvalidDicomError, Exception):\n",
    "            continue\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Grouping completed. Identified {len(dicom_groups)} logical series in total.\")\n",
    "    return dicom_groups\n",
    "\n",
    "def get_largest_series_files(all_series: Dict[Tuple, List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Finds the series with the most slices and returns the file list sorted by InstanceNumber.\n",
    "\n",
    "    Args:\n",
    "        all_series (dict): Grouping dictionary returned by group_dicom_files.\n",
    "\n",
    "    Returns:\n",
    "        list: List of DICOM file paths from the series with the most slices, sorted by InstanceNumber.\n",
    "    \"\"\"\n",
    "    if not all_series:\n",
    "        return []\n",
    "\n",
    "    # 1. Find the series with the most slices\n",
    "    # (Key, file list)\n",
    "    max_layers_series_item = None\n",
    "    max_layers = 0\n",
    "\n",
    "    for group_key, file_list in all_series.items():\n",
    "        if len(file_list) > max_layers:\n",
    "            max_layers = len(file_list)\n",
    "            max_layers_series_item = (group_key, file_list)\n",
    "\n",
    "    if not max_layers_series_item:\n",
    "        print(\"No valid series found.\")\n",
    "        return []\n",
    "\n",
    "    target_group_key, target_series_files = max_layers_series_item\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"*** Found series with the most slices *** (Slice count: {max_layers})\")\n",
    "    print(f\"  Modality: {target_group_key[2]}\")\n",
    "    print(f\"  Orientation: {target_group_key[3][:3]}...\")\n",
    "\n",
    "    # 2. Sort the target series (using InstanceNumber)\n",
    "    sorted_files_with_number: List[Tuple[Any, str]] = []\n",
    "    \n",
    "    # Iterate through files to get InstanceNumber\n",
    "    for fp in target_series_files:\n",
    "        try:\n",
    "            ds = pydicom.dcmread(fp, stop_before_pixels=True)\n",
    "            # Sort by InstanceNumber. Use 0 if InstanceNumber doesn't exist.\n",
    "            # Alternatively, consider using ImagePositionPatient[2] for sorting\n",
    "            instance_number = getattr(ds, 'InstanceNumber', 0)\n",
    "            sorted_files_with_number.append((instance_number, fp))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Sort by InstanceNumber\n",
    "    sorted_files_with_number.sort(key=lambda x: x[0])\n",
    "    \n",
    "    final_file_paths = [fp for _, fp in sorted_files_with_number]\n",
    "    \n",
    "    return final_file_paths\n",
    "\n",
    "def get_spacing_by_shape(shape):\n",
    "    \"\"\"\n",
    "    Efficiently maps spacing based on the size of each axis.\n",
    "    Rules:\n",
    "    - Axis > 300: 0.5mm\n",
    "    - 120 < Axis <= 300: 0.55mm  \n",
    "    - 100 < Axis <= 120: 0.75mm\n",
    "    - 80 < Axis <= 100: 1.0mm\n",
    "    - 60 < Axis <= 80: 1.5mm\n",
    "    - 45 < Axis <= 60: 3.0mm\n",
    "    - Axis <= 45: 5.0mm\n",
    "    \"\"\"\n",
    "    spacing = []\n",
    "    for dim in shape:\n",
    "        if dim > 300:\n",
    "            spacing.append(0.5)\n",
    "        elif dim > 120:\n",
    "            spacing.append(0.55)\n",
    "        elif dim > 100:\n",
    "            spacing.append(0.75)\n",
    "        elif dim > 80:\n",
    "            spacing.append(1.0)\n",
    "        elif dim > 60:\n",
    "            spacing.append(1.5)\n",
    "        elif dim > 45:\n",
    "            spacing.append(3.0)\n",
    "        else:\n",
    "            spacing.append(5.0)\n",
    "    return spacing\n",
    "\n",
    "def flip_z(img_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Flip along Z-axis (superior-inferior): corresponds to dim=2\"\"\"\n",
    "    # Shape [B, C, D, H, W], D corresponds to dim=2\n",
    "    return torch.flip(img_tensor, dims=[2])\n",
    "\n",
    "def flip_y(img_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Flip along Y-axis (anterior-posterior): corresponds to dim=3\"\"\"\n",
    "    # Shape [B, C, D, H, W], H corresponds to dim=3\n",
    "    return torch.flip(img_tensor, dims=[3])\n",
    "\n",
    "def flip_x(tensor):\n",
    "    \"\"\"Flip along X-axis (left-right): corresponds to dim=4\"\"\"\n",
    "    # Shape [B, C, D, H, W], W corresponds to dim=4\n",
    "    return torch.flip(tensor, dims=[4])\n",
    "\n",
    "@torch.no_grad()\n",
    "def worker_infer(num_cls_task, tta_batch_size, image_resized, aneurysm_predictor_fold, fold_i, all_fold_mean_logits_by_task):\n",
    "    \"\"\"\n",
    "    image_resized: can be on whatever device, because `image_resized = image_resized.to(device)` \n",
    "        will move it to correct device for inference. \n",
    "    Results are on CPU.\n",
    "    all_fold_mean_logits_by_task: modified inplace in this function.\n",
    "        It is thread-safe if we make sure different threads write to different slots (or sub-slots) of the list.\n",
    "    \"\"\"\n",
    "    device = aneurysm_predictor_fold.device\n",
    "    print(f\"[*] Worker Using Device: {device}\")\n",
    "\n",
    "    # Task 2 (Location Classification) Left-right flip index mapping (13 classes: 0-12)\n",
    "    # [L_ICL, R_ICL, L_SCL, R_SCL, L_MCA, R_MCA, AC, L_AC, R_AC, L_PC, R_PC, BT, OP]\n",
    "    # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n",
    "    # Swap: 1, 0, 3, 2, 5, 4, 6, 8, 7, 10, 9, 11, 12\n",
    "    # Only for Task 2 logits (shape [N_classes=13])\n",
    "    TASK2_SWAP_INDICES = [1, 0, 3, 2, 5, 4, 6, 8, 7, 10, 9, 11, 12]\n",
    "\n",
    "    image_resized = image_resized.to(device)\n",
    "    # ======================================================\n",
    "    # TTA Step 1: Construct all TTA augmented image list\n",
    "    # Add X-axis flip (left-right flip)\n",
    "    # ======================================================\n",
    "    \n",
    "    # 1. Original image\n",
    "    I_orig = image_resized\n",
    "    # 2. Augmentation A: Y-axis flip (anterior-posterior)\n",
    "    I_flip_y = flip_y(I_orig)\n",
    "    # 3. Augmentation B: Z-axis flip (superior-inferior)\n",
    "    I_flip_z = flip_z(I_orig)\n",
    "    # 4. Augmentation C: Y-axis + Z-axis flip\n",
    "    I_flip_yz = flip_y(I_flip_z)\n",
    "    \n",
    "    # 5. Augmentation D: X-axis flip (left-right)\n",
    "    I_flip_x = flip_x(I_orig)\n",
    "    # 6. Augmentation E: X-axis + Y-axis flip\n",
    "    I_flip_xy = flip_x(I_flip_y)\n",
    "    # 7. Augmentation F: X-axis + Z-axis flip\n",
    "    I_flip_xz = flip_x(I_flip_z)\n",
    "    # 8. Augmentation G: X-axis + Y-axis + Z-axis flip\n",
    "    I_flip_xyz = flip_x(I_flip_yz)\n",
    "    \n",
    "    # TTA list (total 8 augmentations)\n",
    "    tta_images = [\n",
    "        I_orig, I_flip_y, I_flip_z, I_flip_yz,\n",
    "        I_flip_x, I_flip_xy, I_flip_xz, I_flip_xyz\n",
    "    ]\n",
    "    \n",
    "    # Mark which augmentations performed X-axis (left-right) flip\n",
    "    # 0: not flipped, 1: X-axis flipped\n",
    "    # Corresponds to tta_images list\n",
    "    x_flip_masks = [0, 0, 0, 0, 1, 1, 1, 1] \n",
    "\n",
    "    # ======================================================\n",
    "    # TTA Step 2: Iterative inference (supports tta_batch_size setting)\n",
    "    # ======================================================\n",
    "    \n",
    "    num_tta_augments = len(tta_images) # 8\n",
    "    # current_fold_tta_logits_by_task[task_i] is a list storing all TTA image logits for that task\n",
    "    current_fold_tta_logits_by_task = [[] for _ in range(num_cls_task)]\n",
    "    \n",
    "    empty_cache(device)\n",
    "    \n",
    "    for i in range(0, num_tta_augments, tta_batch_size):\n",
    "        # Construct current batch of TTA images and corresponding flip flags\n",
    "        batch_tta_images = tta_images[i:i + tta_batch_size]\n",
    "        batch_x_flip_masks = x_flip_masks[i:i + tta_batch_size]\n",
    "        \n",
    "        # Stack into TTA Batch (shape: [tta_batch_size, 1, 224, 224, 224])\n",
    "        image_tta_batch = torch.cat(batch_tta_images, dim=0)\n",
    "\n",
    "        with torch.autocast(device.type, enabled=True) if device.type == 'cuda' else dummy_context():\n",
    "            # predicted_logits_list shape: [task1_logits, task2_logits],\n",
    "            predicted_logits_list = aneurysm_predictor_fold.network(image_tta_batch, only_forward_cls=True)\n",
    "            \n",
    "            # Collect logits for each task\n",
    "            # Task 1 (first task, index 0): No left-right swap (assuming Aneurysm Present)\n",
    "            current_fold_tta_logits_by_task[0].append(predicted_logits_list[0])\n",
    "            \n",
    "            # Task 2 (second task, index 1): Handle left-right swap\n",
    "            current_task2_logits = predicted_logits_list[1]\n",
    "            \n",
    "            processed_logits_list = []\n",
    "            for j, is_x_flipped in enumerate(batch_x_flip_masks):\n",
    "                # If current TTA image performed X-axis flip (left-right flip)\n",
    "                if is_x_flipped == 1:\n",
    "                    # Swap left-right positions for Task 2 results\n",
    "                    flipped_logits = current_task2_logits[j][TASK2_SWAP_INDICES]\n",
    "                    processed_logits_list.append(flipped_logits)\n",
    "                else:\n",
    "                    # Otherwise keep original logits\n",
    "                    processed_logits_list.append(current_task2_logits[j])\n",
    "                    \n",
    "            # Collect processed Task 2 logits\n",
    "            current_fold_tta_logits_by_task[1].append(torch.stack(processed_logits_list))\n",
    "        \n",
    "        del image_tta_batch\n",
    "        empty_cache(device)\n",
    "\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # TTA Step 3: Current Fold result integration (Logits averaging)\n",
    "    # --------------------------------------------------\n",
    "    for task_i in range(num_cls_task):\n",
    "        # Merge all batch logits for current fold into a large [num_tta_augments, num_classes] Tensor\n",
    "        logits_tta = torch.cat(current_fold_tta_logits_by_task[task_i], dim=0)\n",
    "        \n",
    "        # Logits averaging (along dimension 0, i.e., batch dimension)\n",
    "        mean_logit_fold = logits_tta.mean(dim=0, keepdim=False).cpu() # shape: [num_classes]\n",
    "        \n",
    "        # Collect current fold's TTA average logits\n",
    "        all_fold_mean_logits_by_task[task_i][fold_i] = mean_logit_fold\n",
    "    \n",
    "\n",
    "def predict_aneurysm(input_img_np, original_spacing, device, tta_batch_size=2):\n",
    "    \"\"\"\n",
    "    Aneurysm prediction function (with left-right flip TTA and left-right swapping for Task 2 results)\n",
    "    \n",
    "    Args:\n",
    "        input_img_np: Input image numpy array\n",
    "        original_spacing: Original image spacing (x, y, z)\n",
    "        device: Computing device (cpu/cuda)\n",
    "    \n",
    "    Returns:\n",
    "        timepoint_probs: Classification probability array [task2 probabilities, task1 probabilities]\n",
    "    \"\"\"\n",
    "    # Stage 1: Vessel ROI prediction\n",
    "    stage_1_target_spacing = np.array([1, 0.55, 0.5])\n",
    "    with torch.no_grad():\n",
    "        z_min_final, z_max_final, y_min_final, y_max_final, x_min_final, x_max_final = GLOBAL_VESSEL_ROI_PREDICTOR.predict_from_multi_axial_slices(\n",
    "            input_img_np, original_spacing, stage_1_target_spacing, max_batch_size=16\n",
    "        )\n",
    "\n",
    "    # Crop image to ROI region\n",
    "    img_cropped_np = input_img_np[0][z_min_final:z_max_final, y_min_final:y_max_final, x_min_final:x_max_final][None]\n",
    "\n",
    "    del input_img_np \n",
    "\n",
    "    img_cropped_np = np.ascontiguousarray(img_cropped_np)\n",
    "\n",
    "    # Stage 2: Aneurysm classification\n",
    "    img_cropped_tensor = torch.from_numpy(img_cropped_np).half().to(device)  # HOUJING: added half()\n",
    "\n",
    "    # Image normalization\n",
    "    image_normed = (img_cropped_tensor - img_cropped_tensor.mean()) / img_cropped_tensor.std().clip(1e-8)\n",
    "\n",
    "    del img_cropped_tensor\n",
    "\n",
    "    dst_shape = [224, 224, 224]\n",
    "    \n",
    "    # Image resampling\n",
    "    image_resized = torch.nn.functional.interpolate(\n",
    "        image_normed[None], size=dst_shape, mode='trilinear', align_corners=True\n",
    "    )\n",
    "\n",
    "    del image_normed\n",
    "\n",
    "    # Initialize a list to store logits for each TTA image\n",
    "    # all_fold_mean_logits_by_task[task_i] is a list storing all TTA image logits for that task\n",
    "    num_cls_task = 2\n",
    "    n_folds = 2\n",
    "    all_fold_mean_logits_by_task = [[None for _ in range(n_folds)] for _ in range(num_cls_task)]\n",
    "\n",
    "     # ======================================================\n",
    "    # Inference for each fold model in parallel\n",
    "    # ======================================================\n",
    "    futures = []\n",
    "    for fold_i, aneurysm_predictor_fold in enumerate(GLOBAL_ANEURYSM_PREDICTOR_ALL_FOLDS):\n",
    "        # This runs asynchronously\n",
    "        futures.append(executor.submit(worker_infer, num_cls_task, tta_batch_size, image_resized, aneurysm_predictor_fold, fold_i, all_fold_mean_logits_by_task))\n",
    "    # Iterating over as_completed(futures) ensures all tasks finish before moving on\n",
    "    for future in as_completed(futures):\n",
    "        pass\n",
    "        \n",
    "    # ======================================================\n",
    "    # Final result integration (averaging TTA mean logits across all folds)\n",
    "    # ======================================================\n",
    "    \n",
    "    aggregated_probs_list = []\n",
    "    \n",
    "    for task_i in range(num_cls_task):\n",
    "        logits_tta = torch.stack(all_fold_mean_logits_by_task[task_i], dim=0)\n",
    "        \n",
    "        mean_logit = logits_tta.mean(dim=0, keepdim=False)\n",
    "        \n",
    "        # Convert to final probabilities using Sigmoid\n",
    "        final_prob = torch.sigmoid(mean_logit)\n",
    "        \n",
    "        aggregated_probs_list.append(final_prob.to('cpu').numpy().flatten())\n",
    "\n",
    "    # Merge probabilities: Task2 first, Task1 last (consistent with original code)\n",
    "    task1_probs = aggregated_probs_list[0] # Task 1 (existence)\n",
    "    task2_probs = aggregated_probs_list[1] # Task 2 (location)\n",
    "\n",
    "    return np.concatenate([task2_probs, task1_probs], axis=0)\n",
    "\n",
    "# ==================================================\n",
    "# 2. Two-Stage Inference\n",
    "# ==================================================\n",
    "def process_single_timepoint(orig_nii, time_index=None):\n",
    "    \"\"\"\n",
    "    Process data for a single timepoint.\n",
    "    \"\"\"\n",
    "    # Extract data for the specified timepoint\n",
    "    if time_index is not None and orig_nii.ndim == 4 and orig_nii.shape[3] > 1:\n",
    "        orig_data = orig_nii.get_fdata()[:, :, :, time_index]\n",
    "        orig_nii = nib.Nifti1Image(orig_data, orig_nii.affine, orig_nii.header)\n",
    "    \n",
    "    # Reorient to standard space\n",
    "    img_orient = reorient_nii(orig_nii, targ_aff=\"LPS\")\n",
    "    input_img_np = img_orient.get_fdata()\n",
    "    \n",
    "    input_img_np = input_img_np.transpose(2, 1, 0)[None]\n",
    "    original_spacing = img_orient.header.get_zooms()[:3]\n",
    "\n",
    "    return predict_aneurysm(input_img_np, original_spacing, DEVICE, tta_batch_size=2)\n",
    "\n",
    "# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n",
    "def predict(series_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Make a prediction for a given DICOM series path.\n",
    "    This function consolidates the core prediction logic into the required format.\n",
    "    \"\"\"\n",
    "    series_id = os.path.basename(series_path)\n",
    "    \n",
    "    # Step 1: Quickly retrieve file list\n",
    "    dicom_files = []\n",
    "    for root, _, files in os.walk(series_path):\n",
    "        for file in files:\n",
    "            # Simplified file type checking\n",
    "            if file.endswith(('.dcm', '.DCM')) or ('.' not in file and len(file) > 1):\n",
    "                dicom_files.append(os.path.join(root, file))\n",
    "    \n",
    "    if len(dicom_files) == 0:\n",
    "        probs = np.ones(len(LABEL_COLS)) * 0.5\n",
    "\n",
    "    elif len(dicom_files) == 1:\n",
    "        \n",
    "        ds = pydicom.dcmread(dicom_files[0], force=True)\n",
    "        \n",
    "        # Get pixel array\n",
    "        pixel_array = ds.pixel_array\n",
    "        # Pixel array shape: (150, 528, 528)\n",
    "        # Computed spacing: [0.55, 0.5, 0.5]\n",
    "        \n",
    "        input_img_np = pixel_array[None]\n",
    "        spacing = get_spacing_by_shape(pixel_array.shape)\n",
    "        original_spacing = spacing[::-1]\n",
    "\n",
    "        print(f\"Computed spacing: {spacing}\")\n",
    "\n",
    "        ''' \n",
    "        Add exception handling\n",
    "        '''\n",
    "        # Filter thick-slice T2 data\n",
    "        if spacing[0] >= 3:\n",
    "            print('Detected thick-slice T2 data, performing orientation analysis...')\n",
    "            \n",
    "            # ========== Inference from 2D numpy array ==========\n",
    "            print(\"\\nInference from 2D numpy array\")     \n",
    "            img_3d = pixel_array  # Directly use the loaded pixel_array\n",
    "            D, _ , _ = pixel_array.shape\n",
    "            slice_idx = D // 2\n",
    "            slice_2d = img_3d[slice_idx, :, :]\n",
    "            \n",
    "            plane_label, plane_id, plane_conf, plane_probs = CLS_2D_PREDICTOR.inference_from_slice(\n",
    "                slice_2d=slice_2d)\n",
    "            \n",
    "            # Exception handling correction\n",
    "            if len(pixel_array.shape) == 3 and plane_id != 0:\n",
    "                print('Entering exception handling...')\n",
    "                pixel_array, spacing = correct_orientation(pixel_array, spacing, plane_id)\n",
    "                print(f\"Corrected pixel array shape: {pixel_array.shape}\")\n",
    "                print(f\"Corrected spacing: {spacing}\")\n",
    "                input_img_np = pixel_array[None]\n",
    "                original_spacing = spacing[::-1]\n",
    "\n",
    "        probs = predict_aneurysm(input_img_np, original_spacing, DEVICE, tta_batch_size=2)\n",
    "        \n",
    "    else:\n",
    "        try: \n",
    "            orig_nii = dicom2nifti.dicom_series_to_nifti(series_path, None, reorient_nifti=False)['NII']\n",
    "        except:\n",
    "            all_series = group_dicom_files(series_path)\n",
    "    \n",
    "            # Find the series with the most slices and sort\n",
    "            largest_series_files = get_largest_series_files(all_series)\n",
    "    \n",
    "            # Now largest_series_files contains the required DICOM file paths sorted by InstanceNumber\n",
    "            if largest_series_files:\n",
    "                print(f\"\\nFinal target series file count: {len(largest_series_files)}\")\n",
    "                # You can now use this list for subsequent image loading and processing\n",
    "    \n",
    "            largest_series_path = \"/kaggle/largest_series_tmp_path\"\n",
    "    \n",
    "            if os.path.exists(largest_series_path):\n",
    "                shutil.rmtree(largest_series_path)\n",
    "            os.makedirs(largest_series_path)\n",
    "    \n",
    "            # 4. Copy files\n",
    "            for file_path in largest_series_files:\n",
    "                filename = os.path.basename(file_path)\n",
    "                dest_path = os.path.join(largest_series_path, filename)\n",
    "                shutil.copy2(file_path, dest_path)\n",
    "    \n",
    "            orig_nii = dicom2nifti.dicom_series_to_nifti(largest_series_path, None, reorient_nifti=False)['NII']\n",
    "            \n",
    "        # Process multi-timepoint data\n",
    "        if orig_nii.ndim == 4 and orig_nii.shape[3] > 1:\n",
    "            # Get number of timepoints\n",
    "            t = orig_nii.shape[3]\n",
    "            \n",
    "            # Store prediction probabilities for each timepoint\n",
    "            all_timepoint_probs = []\n",
    "            \n",
    "            # Perform inference for each timepoint\n",
    "            for t_i in range(t):\n",
    "                print(f\"Processing timepoint {t_i + 1}/{t}\")\n",
    "                \n",
    "                # Use the reused processing function, passing time index\n",
    "                timepoint_probs = process_single_timepoint(\n",
    "                    orig_nii,\n",
    "                    time_index=t_i\n",
    "                )\n",
    "                all_timepoint_probs.append(timepoint_probs)\n",
    "            \n",
    "            # Combine probabilities from all timepoints, take maximum\n",
    "            all_timepoint_probs = np.array(all_timepoint_probs)  # shape: (T, prob_length)\n",
    "            probs = np.max(all_timepoint_probs, axis=0)\n",
    "            \n",
    "        else:\n",
    "            # Single timepoint processing\n",
    "            probs = process_single_timepoint(\n",
    "                orig_nii\n",
    "            )\n",
    "\n",
    "    pred_df = pl.DataFrame(\n",
    "        data=[probs.tolist()],\n",
    "        schema=LABEL_COLS,\n",
    "        orient='row'\n",
    "    )\n",
    "        \n",
    "    # Perform memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # ----------------------------- IMPORTANT ------------------------------\n",
    "    # You MUST have the following code in your `predict` function\n",
    "    # to prevent \"out of disk space\" errors. This is a temporary workaround\n",
    "    # as we implement improvements to our evaluation system.\n",
    "    shutil.rmtree('/kaggle/shared', ignore_errors=True)\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddf4d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T05:30:35.226984Z",
     "iopub.status.busy": "2025-10-22T05:30:35.226324Z",
     "iopub.status.idle": "2025-10-22T05:30:35.230488Z",
     "shell.execute_reply": "2025-10-22T05:30:35.229873Z"
    },
    "papermill": {
     "duration": 0.008621,
     "end_time": "2025-10-22T05:30:35.231507",
     "exception": false,
     "start_time": "2025-10-22T05:30:35.222886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from time import time\n",
    "# case_paths = [\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10004044428023505108375152878107656647',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10004684224894397679901841656954650085',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10005158603912009425635473100344077317',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10009383108068795488741533244914370182',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10012790035410518400400834395242853657',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10014757658335054766479957992112625961',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10021411248005513321236647460239137906',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10022688097731894079510930966432818105',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10022796280698534221758473208024838831',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10023411164590664678534044036963716636',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10030095840917973694487307992374923817',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10030804647049037739144303822498146901',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10034081836061566510187499603024895557',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10035643165968342618460849823699311381',\n",
    "#     '/kaggle/input/rsna-intracranial-aneurysm-detection/series/1.2.826.0.1.3680043.8.498.10035782880104673269567641444954004745'\n",
    "# ]\n",
    "# st = time()\n",
    "# for p in case_paths:\n",
    "#     this_st = time()\n",
    "#     predict(p)\n",
    "#     print(f\"******** One case done, {time() - this_st:.2f}s ********\")\n",
    "# total_time = time() - st\n",
    "# avg_time = total_time / len(case_paths)\n",
    "# print(f\"All Done, {total_time:.2f}s\")\n",
    "# print(f\"Avg time: {avg_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1775d699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T05:30:35.237147Z",
     "iopub.status.busy": "2025-10-22T05:30:35.236939Z",
     "iopub.status.idle": "2025-10-22T05:31:41.682515Z",
     "shell.execute_reply": "2025-10-22T05:31:41.681741Z"
    },
    "papermill": {
     "duration": 66.449853,
     "end_time": "2025-10-22T05:31:41.683892",
     "exception": false,
     "start_time": "2025-10-22T05:30:35.234039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Worker Using Device: cuda:0\n",
      "[*] Worker Using Device: cuda:1\n",
      "[*] Worker Using Device: cuda:0[*] Worker Using Device: cuda:1\n",
      "\n",
      "[*] Worker Using Device: cuda:0\n",
      "[*] Worker Using Device: cuda:1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>SeriesInstanceUID</th><th>Left Infraclinoid Internal Carotid Artery</th><th>Right Infraclinoid Internal Carotid Artery</th><th>Left Supraclinoid Internal Carotid Artery</th><th>Right Supraclinoid Internal Carotid Artery</th><th>Left Middle Cerebral Artery</th><th>Right Middle Cerebral Artery</th><th>Anterior Communicating Artery</th><th>Left Anterior Cerebral Artery</th><th>Right Anterior Cerebral Artery</th><th>Left Posterior Communicating Artery</th><th>Right Posterior Communicating Artery</th><th>Basilar Tip</th><th>Other Posterior Circulation</th><th>Aneurysm Present</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1.2.826.0.1.3680043.8.498.1002…</td><td>0.005913</td><td>0.016983</td><td>0.070435</td><td>0.033966</td><td>0.812012</td><td>0.998047</td><td>0.034088</td><td>0.025558</td><td>0.033539</td><td>0.015015</td><td>0.023956</td><td>0.012527</td><td>0.006958</td><td>0.99707</td></tr><tr><td>&quot;1.2.826.0.1.3680043.8.498.1007…</td><td>0.222412</td><td>0.388672</td><td>0.05899</td><td>0.115356</td><td>0.037903</td><td>0.209473</td><td>0.048767</td><td>0.008125</td><td>0.015961</td><td>0.028656</td><td>0.066345</td><td>0.82373</td><td>0.397705</td><td>0.733887</td></tr><tr><td>&quot;1.2.826.0.1.3680043.8.498.1005…</td><td>0.682617</td><td>0.12085</td><td>0.476074</td><td>0.183594</td><td>0.052521</td><td>0.009781</td><td>0.020218</td><td>0.00202</td><td>0.001768</td><td>0.028275</td><td>0.00634</td><td>0.00082</td><td>0.076965</td><td>0.330322</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 15)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ SeriesIns ┆ Left Infr ┆ Right Inf ┆ Left Supr ┆ … ┆ Right     ┆ Basilar   ┆ Other     ┆ Aneurysm │\n",
       "│ tanceUID  ┆ aclinoid  ┆ raclinoid ┆ aclinoid  ┆   ┆ Posterior ┆ Tip       ┆ Posterior ┆ Present  │\n",
       "│ ---       ┆ Internal  ┆ Internal  ┆ Internal  ┆   ┆ Communica ┆ ---       ┆ Circulati ┆ ---      │\n",
       "│ str       ┆ Car…      ┆ Ca…       ┆ Car…      ┆   ┆ ting …    ┆ f64       ┆ on        ┆ f64      │\n",
       "│           ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆           ┆ ---       ┆          │\n",
       "│           ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆           ┆ f64       ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 1.2.826.0 ┆ 0.005913  ┆ 0.016983  ┆ 0.070435  ┆ … ┆ 0.023956  ┆ 0.012527  ┆ 0.006958  ┆ 0.99707  │\n",
       "│ .1.368004 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3.8.498.1 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 002…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 1.2.826.0 ┆ 0.222412  ┆ 0.388672  ┆ 0.05899   ┆ … ┆ 0.066345  ┆ 0.82373   ┆ 0.397705  ┆ 0.733887 │\n",
       "│ .1.368004 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3.8.498.1 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 007…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 1.2.826.0 ┆ 0.682617  ┆ 0.12085   ┆ 0.476074  ┆ … ┆ 0.00634   ┆ 0.00082   ┆ 0.076965  ┆ 0.330322 │\n",
       "│ .1.368004 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3.8.498.1 ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 005…      ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway()\n",
    "    display(pl.read_parquet('/kaggle/working/submission.parquet'))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13851420,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 8385277,
     "sourceId": 13228807,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8537699,
     "sourceId": 13450569,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 470548,
     "modelInstanceId": 454330,
     "sourceId": 605676,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 470583,
     "modelInstanceId": 454370,
     "sourceId": 605720,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 477182,
     "modelInstanceId": 461427,
     "sourceId": 614947,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 146.613683,
   "end_time": "2025-10-22T05:31:45.493698",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-22T05:29:18.880015",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
